<!DOCTYPE html>
<html lang="en">

<head>
    <!-- meta -->
    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-164778559-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', 'UA-164778559-1', { cookie_flags: 'max-age=7200;secure;samesite=none' });
</script>
<!-- end google analytics -->

<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
  })(window, document, 'script', 'dataLayer', 'GTM-MX6VS7T');</script>
<!-- End Google Tag Manager -->
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="데이터분석 및 시각화 기술을 다루고 다른 tech 개념과 기술을 설명하는 블로그입니다."/>
    <meta property="og:title" content="Data Traveler - Data Tech Blog" />
    <meta property="og:image" content="https://ifh.cc/g/wRy7wk.png" />
    <meta property="og:description" content="데이터분석 및 시각화 기술을 다루고 다른 tech 개념과 기술을 설명하는 블로그입니다."/>
    <meta name="naver-site-verification" content="9587fb2e71e511ea1c87a0881159739147ed08cc" />
    <!-- css -->
    <link rel="stylesheet" type="text/css" href="/static/css/reset.css">
    <link rel="stylesheet" href="/pygments.css">
    <link rel="shortcut icon" href="/static/img/favicon.ico">
    <link rel="stylesheet" type="text/css" href="/static/css/style.css">
    <!-- font -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.2/css/all.min.css" rel="stylesheet">
    <!-- js -->
    
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src='//unpkg.com/valine/dist/Valine.min.js'></script>
<script src="https://developers.kakao.com/sdk/js/kakao.js"></script>
<script type="text/javascript" src="/static/js/share.js"></script>

    <title>Anywhere</title>
</head>

<body>
    <!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MX6VS7T" height="0" width="0"
        style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
    <header>
    <div class="title">
        <div class="title-subcont">
            <a href="/">Data Traveler</a>
            <button class="btn" type="button" onclick="window.open('/resume.html','_blank');">Resume CV</button>
            
        </div>
        <h6>Tantum videmus quantum scimus</h6>
    </div>
</header>
    <div id="content">
    
<div id="post">
    <hr>
    <h3>2021-07-31</h3>
    <p class="tag"><strong>Author:</strong> 98hyun</p>
    <p class="tag"><strong>Published:</strong> 2021-07-31</p>
    <p class="tag">
	
	    <strong>Tags:</strong> 
	    
	        <a href="/tag/%EC%8B%9C%EA%B0%81%ED%99%94.html">시각화</a>    
	    
	        <a href="/tag/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D.html">데이터분석</a>    
	    
	        <a href="/tag/%EB%94%A5%EB%9F%AC%EB%8B%9D.html">딥러닝</a>    
	    
	
</p>
    
<p><strong>Share : 
    <span>
        <a id="kakao-link-btn" href="javascript:kakao()">
            <img width="40" height="40" 
                src="https://developers.kakao.com/assets/img/about/logos/kakaolink/kakaolink_btn_medium.png" />
        </a>
    </span>
    <span>
        <script type="text/javascript" src="https://ssl.pstatic.net/share/js/naver_sharebutton.js"></script>
        <script type="text/javascript">
            new ShareNaver.makeButton({
                "type": "f", "title": document.querySelector('meta[property="og:title"]')["content"],
                "url": window.location.href });
        </script>
    </span>
</strong></p>

    <hr>
    <p><br></p>
<h3 style="border-left: solid 3px #0E6073;"><span style="background-color:#2e3f59"></span> &nbsp; autoviz </h3>

<p>이번엔 데이터 분석을 쉽게 할 수 있는 라이브러리들을 만났다. 버전도 따로 없어서 일단 사용한다.<br />
정말 간단하게 볼 수 있는 정도다. </p>
<p>autoviz로 그린 그림은 그렇게 도움이 될지 모르겠다. pandas를 통해 데이터를 조작해가며 확인하는게 더 중요할 듯 하다.</p>
<details><summary>code</summary><blockquote><pre><code>

## https://github.com/AutoViML/AutoViz 참고
## pip install autoviz 
from autoviz.AutoViz_Class import AutoViz_Class

AV = AutoViz_Class()

import pandas as pd

## 지금 만지고 있는 파일과 train.csv 파일이 같은 경로에 있어서 train.csv 라고만 썼다. 
filename = "train.csv" ## 파일 경로를 쓴다.
sep = ","
dft = AV.AutoViz(
    filename,
    sep=",",
    depVar="credit",
    dfte=None,
    header=0,
    verbose=0, ## verbose 0 이면 출력, 2면 저장
    lowess=False,
    chart_format="png",
    max_rows_analyzed=150000,
    max_cols_analyzed=30,
)

</code></pre></blockquote></details>

<p><img alt="" src="https://ifh.cc/g/EDaBmt.png" />
<img alt="" src="https://ifh.cc/g/HlcHek.png" /></p>
<p><br></p>
<h3 style="border-left: solid 3px #0E6073;"><span style="background-color:#2e3f59"></span> &nbsp; qgrid </h3>

<p>qgrid는 jupyter에서만 사용할 수 있는 셀에서 손쉽게 열과 행을 확인한다. 행도 버릴 수 있다면 그렇게 할 수 있는 데이터전처리 라이브러리다. pandas랑 같이 사용 한다.  </p>
<p>밑에 그림은 colab에서 실행한 것을 찍어왔다. local에서 실행하면 browser로 편하게 볼 수 있다. </p>
<details><summary>code</summary><blockquote><pre><code>

## https://github.com/quantopian/qgrid 참고
## pip install qgrid 
# jupyter nbextension enable --py --sys-prefix qgrid - extension을 사용가능하게 한다. 
# jupyter nbextension enable --py --sys-prefix widgetsnbextension - 만약 이 문장을 처음에 실행 안했다면 이것만 해도 된다.
import qgrid

import pandas as pd 

df=pd.read_csv('train.csv')

## qgrid 이것밖에 없다. autoviz로 사진 둘러보고 grid로 클릭하며 없애면 괜찮을 것 같다. 
## 대신 열 지우는 방법은 모르겠다. 아직 issue도 많고 버전도 낮아서 많은 보안이 필요할 것 같다.
qgrid_widget=qgrid.show_grid(df,show_toolbar=True)
qgrid_widget

## qgrid_widget.get_changed_df()
## 바꾸고는 바꾼 df를 저장하는 코드다.
</code></pre></blockquote></details>

<p><br></p>
<p><img src="https://ifh.cc/g/I6cXAq.png" width="100%" height="100%"></p>
<h3 style="border-left: solid 3px #0E6073;"><span style="background-color:#2e3f59"></span> &nbsp; dataprep </h3>

<p>데이터 요약 라이브러리다. autoviz랑 비슷하고 혹은 보다 심화되어 정규성 및 분포를 한눈에 알 수 있는 report 라이브러리다. pandas profiling 같은 것이다. </p>
<details><summary>code</summary><blockquote><pre><code>

!pip install dataprep

from dataprep.datasets import load_dataset
from dataprep.eda import create_report,plot
df = load_dataset("titanic")
plot(df)

## show_browser는 local에서만 되는듯 하다.
# create_report(df).show_browser()

## 뒤에 show_browser없으면 그냥 실행하는것 같다.
create_report(df)

</code></pre></blockquote></details>

<p><img src="https://ifh.cc/g/yWM28B.png" width="100%" height="100%">
<img src="https://ifh.cc/g/j74iDL.png" width="100%" height="100%">
<img src="https://ifh.cc/g/JAO7nG.png" width="100%" height="100%"></p>
<p><br></p>
<h3 style="border-left: solid 3px #0E6073;"><span style="background-color:#2e3f59"></span> &nbsp; R </h3>

<p>본인은 miniconda를 잘 사용하고 있어 r도 그렇게 하고 있지만, 컴퓨터 사양이 낮다보니 느릴때도 있고, 버전에 대해서 다운로드할때도 느리곤해서 가끔 colab을 사용한다. https://colab.to/r 에서 할 수 있다. </p>
<p>보통 conda를 많이 사용할 수 있다. 본인도 그렇다. colab에서 conda를 활용하여 실행하는 방법을 말하려고 한다. </p>
<details><summary>code</summary><blockquote><pre><code>

## 하나씩 해보면서 어떤 역할인지 확인하면서 공부한다. linux공부도 된다. 
! wget https://repo.anaconda.com/miniconda/Miniconda3-py37_4.9.2-Linux-x86_64.sh
! chmod +x Miniconda3-py37_4.9.2-Linux-x86_64.sh
! bash ./Miniconda3-py37_4.9.2-Linux-x86_64.sh -b -f -p /usr/local

import sys
sys.path.append('/usr/local/lib/python3.7/site-packages/')

## 
!which conda  # should return /usr/local/bin/conda

!conda create -n r_envname r-essentials r-base ## 파이썬은 python==3.7하면 된다. 

!source activate r_envname && conda env list

## !conda install --channel conda-forge --file requirements.txt --yes 
</code></pre></blockquote></details>

<p><br></p>
<h3 style="border-left: solid 3px #0E6073;"><span style="background-color:#2e3f59"></span> &nbsp; yolov5 </h3>

<p>yolov5의 yolo는 you look only once의 줄임말로 '한번만 봤다' 이런 뜻이다. 실시간 객체 탐지 과정으로 이미지를 한번만 보고 빠른속도로 (정확도는 어느정도 포기하지만) 추론 가능하다. 영상을 탐지할 수도, 이미지를 탐지할 수도 있다. </p>
<p>이번 내용은 <a href="https://roboflow.com/">roboflow</a>라는 곳에서 데이터를 가져와 colab에서 실행시킨 것이다.  </p>
<p>최근 스터디에서 detection을 공부했는데 너무 어렵다. 내 수준에서의 이해가 전혀 안됐기 때문에 github에서 가져와서 사용할 수 있는지만 테스트 해봤다. </p>
<p>이미지는 train_test_split으로 랜덤하게 나뉘기 때문에 런타임이 실행 될 때마다 바뀐다. 영상은 나온 결과물을 잘라서 가져왔다. </p>
<details><summary>code</summary><blockquote><pre><code>

## 위 링크는 사람마다 다르다. 가입을 해야 가능할 것이다. 
!curl -L "https://public.roboflow.com/ds/3J4wY53d2a?key=3uhArsiGxJ" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip

!git clone https://github.com/ultralytics/yolov5.git

%cd yolov5
!pip install -r requirements.txt

## 파일 생성 및 옮기기
%cd /content/

%mkdir datasets

%mv /content/export/ /content/datasets/export
%mv /content/README.dataset.txt /content/datasets/README.dataset.txt
%mv /content/README.roboflow.txt /content/datasets/README.roboflow.txt
%mv /content/data.yaml /content/datasets/data.yaml

## yaml 파일 수정
import yaml

with open('/content/datasets/data.yaml') as f:
    data = yaml.load(f, Loader=yaml.FullLoader)

with open('/content/datasets/data.yaml','w') as f:
    data['train']='/content/datasets/train.txt'
    data['val']='/content/datasets/val.txt'
    yaml.dump(data,f)

## train과 validation으로 나누기.
from glob import glob 

img_list=glob('/content/datasets/export/images/*.jpg')
print(len(img_list))

from sklearn.model_selection import train_test_split

train_img,val_img=train_test_split(img_list,test_size=0.15,random_state=71)

print(len(train_img),len(val_img))

with open('/content/datasets/train.txt','w') as f:
    f.write('\n'.join(train_img)+'\n')

with open('/content/datasets/val.txt','w') as f:
    f.write('\n'.join(val_img)+'\n')

%cd /content/yolov5/

## img 크기는 416 .md 파일에 다 써있다. 
## batchsize와 epoch만 해준다. 
!python train.py --img 416 --batch 16 --epochs 5 --data /content/datasets/data.yaml --weights yolov5s.pt

## 이미지 추론
from PIL import Image
import os

val_img_path=val_img[1]

## 여기서 runs/train/exp 폴더는 계속 훈련할 때 마다 바뀌기 때문에 처음에 한번에 훈련이 성공하면 exp
## 안되면 exp2 exp3 이런식으로 간다. colab의 폴더를 직접 찾아가보면 무슨말인지 안다.
!python detect.py --source "{val_img_path}" --weights /content/yolov5/runs/train/exp/weights/best.pt --img 416

Image.open(os.path.join('runs/detect/exp4',os.path.basename(val_img_path)))

## 유튜브(영상) 추론
## 이것도 같다. exp 폴더가 잘 있는지 경로 틀리는 이유가 제일 크다.
!python detect.py --weights /content/yolov5/runs/train/exp/weights/best.pt --source 'https://youtu.be/nAH7APL-FsM' 

</code></pre></blockquote></details>

<p><img alt="" src="https://ifh.cc/g/2C7gsd.jpg" />
<img alt="" src="https://ifh.cc/g/8ohLUX.gif" /></p>
<p><br></p>
<h3 style="border-left: solid 3px #0E6073;"><span style="background-color:#2e3f59"></span> &nbsp; kobert </h3>

<p><a href="http://www.scic2021.com/">삼성카드</a> 데이터 분석에서 Track 1 상담원 및 삼성카드 서비스에 대한 만족/불만족 피드백 분류 모델 개발 분야를 스터디 팀원들과 도전했다. </p>
<p>그 중 사용한 kobert 모델을 어떻게 적용했는지 링크와 함께 소개하고자 한다. <br />
https://colab.research.google.com/drive/1FtbkgceU8jrByyEbwBQ1vDsUnVSc9mOI?usp=sharing</p>
<p>여기에는 kobert말고 kcelectra, kcbert도 있다. 모두 비슷한 원리에서 구현되어진다.</p>
<details><summary>code</summary><blockquote><pre><code>

## nlp 작업 틀.

## 1. sentence를 tokenizer로 자르기.
## 2. token들을 embedding 하기.
## 3. embedding 된 데이터들로 modeling 하기
## 4. loss function을 정의하여 학습

## 중간중간에 데이터셋 만들어주고, embedding도 vocab에 대해서 미리 정의해줘야하고 할일이 있지만, 큰 틀이다. 

## 참고
## https://github.com/SKTBrain/KoBERT

## train 과정
## fine-tuning 코드 중 일부다. 

for e in range(num_epochs):
    train_acc = 0.0
    test_acc = 0.0
    kobert_model.train()
    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):
        optimizer.zero_grad()
        token_ids = token_ids.long().to(device)
        segment_ids = segment_ids.long().to(device)
        valid_length= valid_length
        label = label.long().to(device)
        ## kobert model은 token_ids와 valid_length, segment_ids를 받는다.
        out = kobert_model(token_ids, valid_length, segment_ids)
        loss = loss_fn(out, label)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(kobert_model.parameters(), max_grad_norm)
        optimizer.step()
        scheduler.step()  # Update learning rate schedule
        train_acc += calc_accuracy(out, label)
        if batch_id % log_interval == 0:
            print("epoch {} batch id {} loss {} train acc {}".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))
    print("epoch {} train acc {}".format(e+1, train_acc / (batch_id+1)))
    kobert_model.eval()
    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):
        token_ids = token_ids.long().to(device)
        segment_ids = segment_ids.long().to(device)
        valid_length= valid_length
        label = label.long().to(device)
        out = kobert_model(token_ids, valid_length, segment_ids)
        test_acc += calc_accuracy(out, label)
    print("epoch {} test acc {}".format(e+1, test_acc / (batch_id+1)))

</code></pre></blockquote></details>

<p><br></p>
    <div id="vcomment"></div>
</div>
<script>
    new Valine({
        el: '#vcomment',
        appId: 'pMCP910pdFNjniAUKmcPWSzx-MdYXbMMI',
        appKey: 'kywgNcY0JmhAJ5UxQ1yPMl6c',
        meta: ['nick', 'mail'],
        lang: 'en',
        pageSize: 7
    });
</script>

    </div>
    <footer>
    <h6>98hyun © 2021 Powered by Flask</h6>
</footer>
</body>

</html>